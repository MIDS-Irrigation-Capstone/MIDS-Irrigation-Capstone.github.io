<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Overall Results | SimCLR-S2</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Comparison of SimCLR-S2 to Supervised Baseline"><meta name=generator content="Hugo 0.81.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/ananke/dist/main.css_5c99d70a7725bacd4c701e995b969fea.css><link rel=stylesheet href=/css/main.css><meta property="og:title" content="Overall Results"><meta property="og:description" content="Comparison of SimCLR-S2 to Supervised Baseline"><meta property="og:type" content="article"><meta property="og:url" content="https://mids-irrigation-capstone.github.io/results/overall/"><meta property="article:section" content="results"><meta property="article:published_time" content="2021-04-10T00:00:00+00:00"><meta property="article:modified_time" content="2021-04-10T00:00:00+00:00"><meta itemprop=name content="Overall Results"><meta itemprop=description content="Comparison of SimCLR-S2 to Supervised Baseline"><meta itemprop=datePublished content="2021-04-10T00:00:00+00:00"><meta itemprop=dateModified content="2021-04-10T00:00:00+00:00"><meta itemprop=wordCount content="1332"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Overall Results"><meta name=twitter:description content="Comparison of SimCLR-S2 to Supervised Baseline"></head><body class="ma0 avenir bg-near-white"><header class="cover bg-top" style=background-image:url(https://mids-irrigation-capstone.github.io/images/results_feature.jpg)><div class="pb3-m pb6-l bg-black-60 article-header"><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib site-title">SimCLR-S2</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/data/ title="Data page">Data</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/models/ title="Models page">Models</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/results/ title="Results page">Results</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/team/ title="Team page">Team</a></li></ul><a href=https://github.com/MIDS-Irrigation-Capstone/Spring2021 target=_blank class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel=noopener aria-label="follow on Github——Opens in a new window"><svg height="32" style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" width="8" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></span></a></div></div></nav><div class="tc-l pv6 ph3 ph4-ns"><h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Overall Results</h1><h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">Comparison of SimCLR-S2 to Supervised Baseline</h2></div></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">RESULTS</aside><h1 class="f1 athelas mt3 mb1">Overall Results</h1><div class="pa3 toc nested-links w-90"><p class="f5 b mb3">Contents</p><nav id=TableOfContents><ul><li><a href=#simclr-s2-vs-baseline>SimCLR-S2 vs Baseline</a></li><li><a href=#baseline-results>Baseline Results</a></li><li><a href=#simclr-s2-finetune-results>SimCLR-S2 Finetune Results</a></li><li><a href=#simclr-s2-distillation-results>SimCLR-S2 Distillation Results</a></li><li><a href=#other-observations>Other Observations</a></li></ul></nav></div></header><div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-90"><p>We performed various experiments by varying the pretraining dataset, type of optimizer used, target labeling scheme, training data size, type of CNN architecture etc. During the course of this research, we ended up 339 working models. In this section we will compare and contrast the performance of our supervised models with SimCLR-S2 models. Table 1 below gives a split up of our experiments</p><table><caption class="serif f4"><strong>Table 1:</strong> <em>Experiments Tally</em></caption><thead><tr><th style=text-align:left>Experiment description</th><th style=text-align:left>Experiment type</th><th style=text-align:center>Total Number</th></tr></thead><tbody><tr><td style=text-align:left>baseline</td><td style=text-align:left>supervised</td><td style=text-align:center>120</td></tr><tr><td style=text-align:left>pretrain</td><td style=text-align:left>SimCLR-S2</td><td style=text-align:center>3</td></tr><tr><td style=text-align:left>finetune</td><td style=text-align:left>SimCLR-S2</td><td style=text-align:center>36</td></tr><tr><td style=text-align:left>distill</td><td style=text-align:left>SimCLR-S2</td><td style=text-align:center>180</td></tr><tr><td style=text-align:left>total experiments</td><td style=text-align:left>all</td><td style=text-align:center>339</td></tr></tbody></table><h2 id=simclr-s2-vs-baseline>SimCLR-S2 vs Baseline</h2><p>The following visualizations show how the SimCLR-S2 model performed compared to the baseline models. The comparison represents the average scores among each architecture for the baseline models as well as the average scores across each distill architecture and pretrained model for the SimCLR-S2 models. When using a small fraction of data the SimCLR-S2 model appears to outperform the baseline.</p><!doctype html><meta charset=utf-8><script src=https://d3js.org/d3.v4.js></script><div><figure><select id=selectvizBasic></select><p style=text-align:center;opacity:0 id=scorevizBasic>Score:</p><div id=vizBasic></div><figcaption><strong>Figure 1:</strong> <em>Interactive visualization for accuracy. F1, and AUC scores for SimCLR-S2 versus baseline models. Scores represent an average for each model type with basic labels.</em></figcaption></figure></div><script>var elemWidth=document.getElementById("vizBasic").offsetWidth,margin={top:10,right:140,bottom:50,left:50},width=elemWidth-margin.left-margin.right,height=elemWidth*.6-margin.top-margin.bottom;d3.csv("/data/results_basic.csv",function(c){var h=d3.select("#scorevizBasic"),a=d3.select("#vizBasic").append("svg").attr("width",width+margin.left+margin.right).attr("height",height+margin.top+margin.bottom).append("g").attr("transform","translate("+margin.left+","+margin.top+")"),g=d3.map(c,function(a){return a.score_type}).keys(),b=c.columns.slice(2),m=d3.map(c,function(a){return a.percent_split}).keys(),f=d3.scaleBand().domain(m).range([0,width]).padding([.2]),d,e,n,o,p,k,q,l;a.append("g").attr("transform","translate(0,"+height+")").call(d3.axisBottom(f).tickSize(0)),d=d3.scaleLinear().domain([0,1]).range([height,0]),a.append("g").call(d3.axisLeft(d)),e=d3.scaleBand().domain(b).range([0,f.bandwidth()]).padding([.05]),color=d3.scaleOrdinal(d3.schemeCategory10),n=d3.select("body").append("div").attr("class","tooltip-donut").style("opacity",0),o=function(a){return a.filter(function(a){return a.score_type==g[0]})},p=function(a){return b.map(function(b){return{key:b,value:a[b]}})},k=g[0],q=function(){var a=c.filter(function(a){return a.score_type==k}),d=[],e;for(i=0;i<a.length;i++)for(j=0;j<b.length;j++)e=b[j],d.push({key:e,value:a[i][e]});return d},a.append("g").selectAll("g").data(o(c)).enter().append("g").attr("transform",function(a){return"translate("+f(a.percent_split)+",0)"}).selectAll("rect").data(p).enter().append("rect").attr("class","bar").on('mousemove',function(a,b){d3.select(this).transition().duration('50').attr('opacity','.85'),n.transition().duration(50).style("opacity",1),h.text("Score: "+a.value).style("opacity",1)}).on('mouseout',function(a,b){d3.select(this).transition().duration('50').attr('opacity','1'),h.transition().duration(50).style("opacity",0)}).attr("x",function(a){return e(a.key)}).attr("y",function(a){return d(a.value)}).attr("width",e.bandwidth()).attr("height",function(a){return height-d(a.value)}).attr("fill",function(a){return color(a.key)}),a.append("text").attr("transform","translate("+width/2+" ,"+(height+margin.top+20)+")").style("text-anchor","middle").text("Split Percent - Basic Labels"),a.append("text").attr("transform","rotate(-90)").attr("y",0-margin.left).attr("x",0-height/2).attr("dy","1em").style("text-anchor","middle").text("Score"),l=a.selectAll(".legend").data(b.slice()).enter().append("g").attr("class","legend").attr("transform",function(b,a){return"translate(0,"+a*20+")"}),l.append("rect").attr("x",width+18).attr("width",18).attr("height",18).style("fill",color),l.append("text").attr("x",width+40).attr("y",9).attr("dy",".35em").style("text-anchor","start").text(function(a){return a}),d3.select("#selectvizBasic").selectAll('myOptions').data(g).enter().append('option').text(function(a){return a}).attr("value",function(a){return a});function r(){a.selectAll(".bar").data(q()).transition().duration(1e3).attr("x",function(a){return e(a.key)}).attr("y",function(a){return d(a.value)}).attr("width",e.bandwidth()).attr("height",function(a){return height-d(a.value)}).attr("fill",function(a){return color(a.key)})}d3.select("#selectvizBasic").on("change",function(a){k=d3.select(this).property("value"),r()})})</script><p><em>Figure 2</em> shows the same results but with the expanded labels. Interestingly, the SimCLR-S2 model did not seem to perform as well as it had when trained with the basic labels. The SimCLR-S2 model only outperformed the baseline when looking at the accuracy score on 1 percent labeled data. The F1 and AUC scores were better for the baseline across the board.</p><p>While the baseline seems to perform better on the expanded labels, the difference is much smaller than was seen in the results for basic labels. Many of models trained with the 1 and 3 percent labels exhibited a wide variance in model performance. For example, F1 scores ranged from 0 to 0.82. This is likely due to the random state in each model as it was trained. Further experimentation with an average over multiple training runs may have produced more consistent baseline results for which to compare. The SimCLR-S2 models did not exhibit the same problem with the small amount of labels.</p><!doctype html><meta charset=utf-8><script src=https://d3js.org/d3.v4.js></script><div><figure><select id=selectvizEx></select><p style=text-align:center;opacity:0 id=scorevizEx>Score:</p><div id=vizEx></div><figcaption><strong>Figure 2:</strong> <em>Interactive visualization for accuracy. F1, and AUC scores for SimCLR-S2 versus baseline models. Scores represent an average for each model type with expanded labels.</em></figcaption></figure></div><script>var elemWidth=document.getElementById("vizEx").offsetWidth,margin={top:10,right:140,bottom:50,left:50},width=elemWidth-margin.left-margin.right,height=elemWidth*.6-margin.top-margin.bottom;d3.csv("/data/results_expanded.csv",function(c){var h=d3.select("#scorevizEx"),a=d3.select("#vizEx").append("svg").attr("width",width+margin.left+margin.right).attr("height",height+margin.top+margin.bottom).append("g").attr("transform","translate("+margin.left+","+margin.top+")"),g=d3.map(c,function(a){return a.score_type}).keys(),b=c.columns.slice(2),m=d3.map(c,function(a){return a.percent_split}).keys(),f=d3.scaleBand().domain(m).range([0,width]).padding([.2]),d,e,n,o,p,k,q,l;a.append("g").attr("transform","translate(0,"+height+")").call(d3.axisBottom(f).tickSize(0)),d=d3.scaleLinear().domain([0,1]).range([height,0]),a.append("g").call(d3.axisLeft(d)),e=d3.scaleBand().domain(b).range([0,f.bandwidth()]).padding([.05]),color=d3.scaleOrdinal(d3.schemeCategory10),n=d3.select("body").append("div").attr("class","tooltip-donut").style("opacity",0),o=function(a){return a.filter(function(a){return a.score_type==g[0]})},p=function(a){return b.map(function(b){return{key:b,value:a[b]}})},k=g[0],q=function(){var a=c.filter(function(a){return a.score_type==k}),d=[],e;for(i=0;i<a.length;i++)for(j=0;j<b.length;j++)e=b[j],d.push({key:e,value:a[i][e]});return d},a.append("g").selectAll("g").data(o(c)).enter().append("g").attr("transform",function(a){return"translate("+f(a.percent_split)+",0)"}).selectAll("rect").data(p).enter().append("rect").attr("class","bar").on('mousemove',function(a,b){d3.select(this).transition().duration('50').attr('opacity','.85'),n.transition().duration(50).style("opacity",1),h.text("Score: "+a.value).style("opacity",1)}).on('mouseout',function(a,b){d3.select(this).transition().duration('50').attr('opacity','1'),h.transition().duration(50).style("opacity",0)}).attr("x",function(a){return e(a.key)}).attr("y",function(a){return d(a.value)}).attr("width",e.bandwidth()).attr("height",function(a){return height-d(a.value)}).attr("fill",function(a){return color(a.key)}),a.append("text").attr("transform","translate("+width/2+" ,"+(height+margin.top+20)+")").style("text-anchor","middle").text("Split Percent - Expanded Labels"),a.append("text").attr("transform","rotate(-90)").attr("y",0-margin.left).attr("x",0-height/2).attr("dy","1em").style("text-anchor","middle").text("Score"),l=a.selectAll(".legend").data(b.slice()).enter().append("g").attr("class","legend").attr("transform",function(b,a){return"translate(0,"+a*20+")"}),l.append("rect").attr("x",width+18).attr("width",18).attr("height",18).style("fill",color),l.append("text").attr("x",width+40).attr("y",9).attr("dy",".35em").style("text-anchor","start").text(function(a){return a}),d3.select("#selectvizEx").selectAll('myOptions').data(g).enter().append('option').text(function(a){return a}).attr("value",function(a){return a});function r(){a.selectAll(".bar").data(q()).transition().duration(1e3).attr("x",function(a){return e(a.key)}).attr("y",function(a){return d(a.value)}).attr("width",e.bandwidth()).attr("height",function(a){return height-d(a.value)}).attr("fill",function(a){return color(a.key)})}d3.select("#selectvizEx").on("change",function(a){k=d3.select(this).property("value"),r()})})</script><h2 id=baseline-results>Baseline Results</h2><p>Our baseline results showed that performance improved with size of training data. Initially, each time the data size tripled, accuracy improved by 33%. As expected, these gains plateaued after a certain point. When we increased the training data size by a hundred fold, we saw an accuracy of 0.93. Our results indicated that the F1 score was better for smaller CNN architectures. Inceptionv3 and Xception with roughly 23 million parameters, gave the highest F1-scores at 0.94, while ResNet152 had a F1-score of 0.91. Training from scratch gave us better F1-scores than training using ImageNet weights. This is probably because ImageNet weights were only available for the visible bands of light, while we use all the available multispectral bands for training from scratch.</p><figure><img src=/images/sup_baseline.png alt="Figure 3: Accuracy and F1 scores for supervised baseline"><figcaption><p><strong>Figure 3:</strong> <em>Accuracy and F1 scores for supervised baseline</em></p></figcaption></figure><h2 id=simclr-s2-finetune-results>SimCLR-S2 Finetune Results</h2><p>The results from our experiments showed that the SimCLR-S2 models outperformed the supervised baseline for smaller samples. For 1-3% of data splits, the accuracy and F1 scores were better or on par with that of the supervised learning. As the data size increased, the model performance dropped to below that of the supervised baselines. Our SimCLR-S2 models consistently out performed the supervised model that was pretrained with ImageNet weights.</p><figure><img src=/images/fine_tune_acc_f1.png alt="Figure 4: SimCLR-S2 Fine tune performance for various models"><figcaption><p><strong>Figure 4:</strong> <em>SimCLR-S2 Fine tune performance for various models</em></p></figcaption></figure><h2 id=simclr-s2-distillation-results>SimCLR-S2 Distillation Results</h2><p>The last and final step in our SimCLR-S2 paradigm is the process of distillation to a student model. We took each ResNet152 model finetuned across the various data splits and performed distill learning to our five model architectures. We found that even for the smallest model Xception, with 22.9 million parameters, there was improved F1 scores for data splits from 3% and above, and with larger models accuracy over the fine-tuned teacher model increased across the board. We suspect that the freezing of the convolutional layers during the fine tuning process resulted in their poorer performance compared to the student models, which had all layers enabled for distill learning. Distillation scores were better than those of supervised baselines for smaller data sizes on many architectures. For the 1% data split, SimCLR-S2 outperformed the baseline across the board. For 3 and 10% data splits there were some architectures where SimCLR-S2 performed better than supervised. Supervised baseline gave better performance than SimCLR-S2 on higher data splits.</p><figure><img src=/images/distill_perf.png alt="Figure 5: Comparison of SimCLR-S2 distillation performance scores with baseline scores"><figcaption><p><strong>Figure 5:</strong> <em>Comparison of SimCLR-S2 distillation performance scores with baseline scores</em></p></figcaption></figure><h2 id=other-observations>Other Observations</h2><p>A distribution of our performance scores across all of our models showed that the SimCLR-S2 models brought the f1, accuracy and AUC scores closer together across different data sizes and CNN architectures. The variability of these scores were much higher with supervised learning. All of our SimCLR-S2 models had an AUC score higher than 0.72, independent of training data or model sizes. The mean AUC score across all SimCLR-S2 models was around 0.83 indicating that SimCLR-S2 models were effective in detecting permanently irrigated land. Supervised learning on the other hand, had models with AUC scores as low as 0.5. We also observed that the mean f1 score between SimCLR-S2 and supervised models was the same.</p><figure><img src=/images/simclr-dist.png alt="Figure 6: Distribution of performance scores for SimCLR-S2 and supervised models"><figcaption><p><strong>Figure 6:</strong> <em>Distribution of performance scores for SimCLR-S2 and supervised models</em></p></figcaption></figure><p>The t-SNE heat map of our SimCLR-S2 pretraining model shows clear indication of dense zones of clustering in both irrigated and non-irrigated classes. We believe that training the models for longer than 50 epochs could potentially help with further separation of these clusters.</p><figure><img src=/images/tsne_heatmaps.png alt="Figure 7: t-SNE heatmaps of our pretrained model trained on BigEarthNet-S2 data"><figcaption><p><strong>Figure 7:</strong> <em>t-SNE heatmaps of our pretrained model trained on BigEarthNet-S2 data</em></p></figcaption></figure><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script></body></html>