<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Models on SimCLR-S2</title><link>https://mids-irrigation-capstone.github.io/models/</link><description>Recent content in Models on SimCLR-S2</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 02 Mar 2017 12:00:00 -0500</lastBuildDate><atom:link href="https://mids-irrigation-capstone.github.io/models/index.xml" rel="self" type="application/rss+xml"/><item><title>SimCLR-S2 Distillation</title><link>https://mids-irrigation-capstone.github.io/models/simclr_distill/</link><pubDate>Sun, 04 Apr 2021 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/models/simclr_distill/</guid><description>In the final set of the SimCLR-S2 process we perform knowledge distillation to attempt to gain similar performance to the big ResNet-152 model with fewer parameters. While it is possible to use some labeled data during distillation we experimented with a completely self-supervised approach.</description></item><item><title>SimCLR-S2 Finetune</title><link>https://mids-irrigation-capstone.github.io/models/simclr_finetune/</link><pubDate>Sun, 04 Apr 2021 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/models/simclr_finetune/</guid><description>We leverage a similar approach as the baseline models to finetune our pretrained contrastive learning model. Multiple CNN architectures and data splits are trained as well as some hyper parameter tuning to maximize performance of the fine-tuned model.</description></item><item><title>SimCLR-S2 Pretraining</title><link>https://mids-irrigation-capstone.github.io/models/simclr_pretrain/</link><pubDate>Sun, 04 Apr 2021 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/models/simclr_pretrain/</guid><description>The SimCLR-S2 pretraining uses completely unlabeled data and as the starting point to train out computer vision model. In addition to the Big Earth Net data, we will run pretraining against sentinel-2 images captured of the California central valley.</description></item><item><title>Baseline Models</title><link>https://mids-irrigation-capstone.github.io/models/supervised_baseline/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/models/supervised_baseline/</guid><description>The goal of this project is to identify if self-supervised learning is a viable option when training on satellite imagery with limited access to labeled data. Our baseline models are trained on a variety of neural network architectures and fractions of labeled data to quantify the performance impact as our fraction of labeled data changes. We will used the results of our baseline models to assess the performance of our self-supervised models with the constraints.</description></item></channel></rss>