<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SimCLR-S2</title><link>https://mids-irrigation-capstone.github.io/</link><description>Recent content on SimCLR-S2</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 10 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://mids-irrigation-capstone.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Overall Results</title><link>https://mids-irrigation-capstone.github.io/results/overall/</link><pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/results/overall/</guid><description>Final results comparison of SimCLR-S2 models versus the supervised baseline.</description></item><item><title>SimCLR-S2 Stage 3: Distillation</title><link>https://mids-irrigation-capstone.github.io/models/simclr_distill/</link><pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/models/simclr_distill/</guid><description>In the final set of the SimCLR-S2 process we perform knowledge distillation to attempt to gain similar performance to the big ResNet152 model with fewer parameters. While it is possible to use some labeled data during distillation we experimented with a completely self-supervised approach.</description></item><item><title>SimCLR-S2 Stage 2: Fine-Tune</title><link>https://mids-irrigation-capstone.github.io/models/simclr_finetune/</link><pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/models/simclr_finetune/</guid><description>We leverage a similar approach as the baseline models to fine tune our pretrained contrastive learning model. We train multipe data splits on our largest CNN architecture ResNet152, to maximize performance of the fine-tuned model.</description></item><item><title>SimCLR-S2 Stage 1: Pretraining</title><link>https://mids-irrigation-capstone.github.io/models/simclr_pretrain/</link><pubDate>Sun, 04 Apr 2021 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/models/simclr_pretrain/</guid><description>The SimCLR-S2 pretraining uses completely unlabeled data and as the starting point to train out computer vision model. In addition to the Big Earth Net data, we will run pretraining against sentinel-2 images captured of the California central valley.</description></item><item><title>Exploratory Analysis</title><link>https://mids-irrigation-capstone.github.io/data/eda/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/data/eda/</guid><description>An exploratory analysis on the data to understand the distributions for irrigated data as well as identify additional features that can help with irrigation detection.</description></item><item><title>Data Sources</title><link>https://mids-irrigation-capstone.github.io/data/sources/</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/data/sources/</guid><description>The data for our research are Sentinel-2 multispectral images of earth&amp;rsquo;s surface reflectance sourced from two archives: BigEarthNet-S2 and Sentinel-2A collection from the Copernicus program.</description></item><item><title>Baseline Models</title><link>https://mids-irrigation-capstone.github.io/models/supervised_baseline/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/models/supervised_baseline/</guid><description>The goal of this project is to identify if self-supervised learning is a viable option when training on satellite imagery with limited access to labeled data. Our baseline models are trained on a variety of neural network architectures and fractions of labeled data to quantify the performance impact as our fraction of labeled data changes. We will used the results of our baseline models to assess the performance of our self-supervised models with the constraints.</description></item><item><title>Chitra Agastya</title><link>https://mids-irrigation-capstone.github.io/team/chitra/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/team/chitra/</guid><description> Data Scientist Operations Manager Web Designer</description></item><item><title>Ian Anderson</title><link>https://mids-irrigation-capstone.github.io/team/ian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/team/ian/</guid><description> Data Scientist Data Engineer Web Designer</description></item><item><title>SimCLR-S2 Distillation</title><link>https://mids-irrigation-capstone.github.io/results/distill/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/results/distill/</guid><description>Data vizualization of performance results for the distilled models.</description></item><item><title>Sirak Ghebremusse</title><link>https://mids-irrigation-capstone.github.io/team/sirak/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mids-irrigation-capstone.github.io/team/sirak/</guid><description> Data Scientist Modeling Specialist Web Designer</description></item></channel></rss>