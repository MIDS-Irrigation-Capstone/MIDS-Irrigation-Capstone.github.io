---
date: 2021-04-05
title: "SimCLR-S2 Stage 2: Fine-Tune"
description: "Supervised Task-Specific Training"
toc: true
featured_image: "/images/farmland-simclr.jpg"
summary_image: "/images/fine-tune.jpg"
summary: We leverage a similar approach as the baseline models to fine tune our pretrained contrastive learning model. We train multipe data splits on our largest CNN architecture ResNet152, to maximize performance of the fine-tuned model.
---

{{< figure src="/images/simclr-2.png" >}}

In this post, we will explore the phase two of the SimCLR methodology, fine-tuning. This phase will use the contrastive model generated from the pre-training step as a starting point to train a supervised model with a smaller labelled dataset specific to the task as hand. We experiment with various size of balanced labeled data generated by under sampling the original BigEarthNet-S2 dataset.

We start the fine tuning step with 3 different SimCLR-S2 pretrained models. For each model, we perform supervised training with 6 different data sizes, each with 2 different class labeling schemes, giving us 36 experiments in all.

{{< figure src="/images/fine_tune_acc_f1.png" caption="**Figure 1:** *SimCLR-S2 Fine tune performance for various models*" >}}

Our fine tuning results showed that the SimCLR-S2 models performed on par or better than the supervised baseline models for smaller data sizes.
